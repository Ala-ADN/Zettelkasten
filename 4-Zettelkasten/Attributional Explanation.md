## Prototype Explanations  
Show typical examples that represent each class, helping visualize what the model "expects."  
Useful for understanding what features or structures are most representative in the data.

## Concept-Based Explanations 
Map internal representations to human-understandable concepts (e.g., "wheel" or "cycle").  
Help interpret decisions in semantically meaningful terms rather than raw features.

## Counterfactual Explanations  
Reveal how small changes to input would alter the model's prediction.  
Useful for understanding decision boundaries and suggesting actionable changes.